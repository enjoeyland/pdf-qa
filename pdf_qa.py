# query_answer.py
from dotenv import load_dotenv
load_dotenv()

import faiss
import numpy as np

from models_factory import EmbeddingModel, LLM, create_embedding, create_llm
from utils import extract_pdf_name

# === ë©€í‹°ëª¨ë‹¬ QA ëª¨ë¸ (BLIP2) ===
# VISION_MODEL_NAME = "Salesforce/blip2-flan-t5-xl"
# vision_processor = Blip2Processor.from_pretrained(VISION_MODEL_NAME, cache_dir="./.cache")
# vision_model = AutoModelForVision2Seq.from_pretrained(VISION_MODEL_NAME, cache_dir="./.cache")

class QueryAnswerer:
    def __init__(self, pdf_path: str, embedder: EmbeddingModel, llm: LLM):
        self.pdf_path = pdf_path
        self.embedder = embedder
        self.llm = llm
        self.VEC_STORE_DIR = f"vector_store/{extract_pdf_name(pdf_path)}"
        self.index = faiss.read_index(f"{self.VEC_STORE_DIR}/index.faiss")
        with open(f"{self.VEC_STORE_DIR}/chunks.npy", "rb") as f:
            self.chunks = np.load(f, allow_pickle=True)
    
    def get_relevant_chunks(self, query: str, top_k: int = 3) -> str:
        # 1. ì§ˆì˜ ì„ë² ë”©
        query_vec = self.embedder.encode([query], is_query=True)

        # 2. ìœ ì‚¬ chunk ê²€ìƒ‰
        _, indices = self.index.search(query_vec, top_k)
        selected_chunks = [self.chunks[i] for i in indices[0]]
        print(f"ìœ ì‚¬ chunk ê²€ìƒ‰ ì™„ë£Œ: {selected_chunks}")
        return selected_chunks


    def _build_prompt(self, query: str, chunks: list) -> str:
        context = "\n\n".join([f"Context {i+1}:\n{chunk['text']}" for i, chunk in enumerate(chunks)])
        prompt = (
            "ë‹¤ìŒì€ PDF ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ë‚´ìš©ì…ë‹ˆë‹¤. "
            "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”. "
            "ëª¨ë¥´ëŠ” ë‚´ìš©ì€ 'No relevant information found.'ë¼ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”.\n\n"
            f"{context}\n\n"
            f"ì§ˆë¬¸: {query}\n"
            "ë‹µë³€:"
        )
        return prompt
    
    def answer_query(self, query: str) -> str:
        selected_chunks = self.get_relevant_chunks(query, top_k=4)

        # 2. LLM ì‘ë‹µ ìƒì„±
        prompt = self._build_prompt(query, selected_chunks)
        answer = self.llm.generate(prompt)
        return answer

        # 3. ì´ë¯¸ì§€ ê¸°ë°˜ ì‘ë‹µ (ìš°ì„ ìˆœìœ„)
        # for chunk in selected_chunks:
        #     if chunk.get("image_path"):
        #         image = Image.open(chunk["image_path"]).convert("RGB")
        #         inputs = vision_processor(images=image, text=query, return_tensors="pt")
        #         with torch.no_grad():
        #             outputs = vision_model.generate(**inputs)
        #         answer = vision_processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
        #         return answer

        # 4. fallback: í…ìŠ¤íŠ¸ chunk ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not selected_chunks:
            return "No relevant information found."
        return selected_chunks[0]["text"]

if __name__ == "__main__":
    PDF_PATH = f"pdf/1.ì œí’ˆì†Œê°œ_ì—ìŠ¤í”¼ë°˜ë„ì²´í†µì‹ .pdf"
    EMB_NAME = "kure"
    LLM_NAME = "gpt-5-mini"

    embedder = create_embedding(EMB_NAME)
    llm = create_llm("custom_openai", model_id=LLM_NAME, response_format="text")
    qa = QueryAnswerer(PDF_PATH, embedder, llm)

    print("PDF ê¸°ë°˜ QA ì‹œìŠ¤í…œì…ë‹ˆë‹¤.")
    while True:
        query = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (exit ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
        if query.lower() == "exit":
            break
        answer = qa.answer_query(query)
        print(f"\nğŸ§  ë‹µë³€: {answer}")
    
    # Sample queries:
    # 1. íšŒì‚¬ëŠ” ì–´ë–¤ í™˜ê²½ë°©ì¹¨ì„ ì¤€ìˆ˜í•˜ê³  ìˆë‚˜ìš”?
    # 2. TO-3P ì œí’ˆì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?
    # 3. 2023ë…„ì—ëŠ” ì–´ë–¤ ì‚¬ê±´ì´ ìˆì—ˆë‚˜ìš”?
    # 4. íšŒì‚¬ ì´ë¦„ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
    # 5. Saw ê³µì •ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜.
    # 6. ì–´ë–¤ í’ˆì§ˆì„ ê°•ì¡°í•˜ê³  ìˆì–´?
    # 7. TO-252 HV ì œí’ˆì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”? 
    #   -> í‘œ ë³‘í•©ì— ë”°ë¼ì„œ ë¬¸ì œê°€ ìˆë‹¤.

    # indexing(gpt-5-mini, kure), retrieve(kure),  generation(gpt-5-mini)
    # indexing      â†’ (1: O, 2: O, 3: O, 4: O, 5: O, 6: â–³, 7: X)
    # retrieve      â†’ (1: O, 2: O, 3: O, 4: X, 5: O, 6: â–³, 7: O)
    # generation    â†’ (1: O, 2: O, 3: â–³, 4: O, 5: O, 6: â–³, 7: O)